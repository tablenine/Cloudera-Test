# example.conf: A single-node Flume configuration

# Name the components on this agent
a1.sources = r1
# Describe/configure the source
a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.batchSize = 100
a1.sources.r1.batchDurationMillis = 2000
a1.sources.r1.kafka.bootstrap.servers = hadoop2:9092
a1.sources.r1.kafka.topics = tableninetopic
a1.sources.r1.kafka.consumer.group.id = custom.g.id

# Describe the sink
a1.sinks.k1.type = hive
a1.sinks.k1.hive.metastore = thrift://hadoop1:9083
a1.sinks.k1.hive.database = rems
a1.sinks.k1.hive.table = ev_oprt_hst
a1.sinks.k1.useLocalTimeStamp = false
a1.sinks.k1.round = true
a1.sinks.k1.roundValue = 1
a1.sinks.k1.roundUnit = hour
a1.sinks.k1.serializer = DELIMITED
a1.sinks.k1.serializer.delimiter = "\t"
a1.sinks.k1.serializer.serdeSeparator = '\t'
a1.sinks.k1.serializer.fieldnames = site_seq,dev_seq,chrgr_no,chrgr_id,chrg_st_time,chrg_ed_time,chrg_time,chrg_pwr_d,chrg_pwr_c,accum_chrg_qnt,chrg_sts,chrgr_sts,cret_dt,updt_dt

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 15000
a1.channels.c1.transactionCapacity = 15000

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
